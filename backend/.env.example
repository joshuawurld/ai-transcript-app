# LLM API Configuration (OpenAI-compatible)
# This app works with ANY OpenAI API-compatible provider!

# OpenRouter (default - uses Claude Sonnet 4.5)
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=your-openrouter-api-key-here
LLM_MODEL=anthropic/claude-sonnet-4.5

# Examples for other providers:
# Ollama (local - runs as separate Docker service): LLM_BASE_URL=http://ollama:11434/v1, LLM_API_KEY=ollama, LLM_MODEL=gemma3:4b
# LM Studio: LLM_BASE_URL=http://localhost:1234/v1, LLM_API_KEY=lm-studio, LLM_MODEL=your-model

# Whisper Configuration (local speech-to-text)
WHISPER_MODEL=base.en

# Calendar Configuration (for agentic workflow)
# Timezone for generated calendar events (IANA timezone format)
CALENDAR_TIMEZONE=America/Los_Angeles

# =============================================================================
# GitHub Integration (OPTIONAL - app works without these)
# =============================================================================
#
# When configured, the app demonstrates TWO approaches to external APIs:
#
# 1. TRANSCRIPT AGENT (REST)
#    - Creates issues from transcript analysis
#    - Uses direct REST API calls to api.github.com
#    - Simple, explicit - we know exactly what we want
#    - See: backend/github_integration.py
#
# 2. ISSUE REVIEW AGENT (MCP)
#    - Reviews existing issues and takes action
#    - Uses MCP for dynamic tool discovery (list_tools())
#    - Agent discovers what actions are available at runtime
#    - Demonstrates when MCP adds value vs REST
#    - See: backend/github_issue_agent.py
#
# SETUP:
# 1. Go to GitHub -> Settings -> Developer Settings -> Personal Access Tokens
# 2. Create a Fine-grained token with Issues (Read and Write) permission
# 3. Set the target repository below
#
GITHUB_TOKEN=ghp_your_personal_access_token_here
GITHUB_ISSUES_REPO=your-username/meeting-tracker

# Issues created by the app get the 'transcript-app' label automatically.
# The Issue Review Agent finds and processes these issues via MCP.
